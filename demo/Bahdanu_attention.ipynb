{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau注意力\n",
    "上下文变量c在任何解码时间同步t都会被c替代，假设输入序列中有T个词元，解码时间步的上下文变量是注意力集中的输出\n",
    "$$c_t=\\sum_{t=1}^T\\alpha(s_{t^(`)-1},h_t)h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义注意力解码器\n",
    "实现神经网络编码器-解码器。只需要重新定义解码器即可。为了更方便学习注意力权重，下面以AttentionDecoder类来定义带有注意力机制解码器的基本接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class AttentionDecoder(d2l.Decoder):\n",
    "    \"\"\" 带有注意力机制解码器的基本接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionDecoder,self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotADirectoryError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们在接下来的Seq2SeqAttentionDeconder类中实现带有Bahdanau注意力的循环神经网络解码器。首先初始化解码器的状态，需要下面的输入：\n",
    "1. 编码器在所有时间步的最终层状态，将作为注意力的键和值：\n",
    "2. 上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态。\n",
    "3. 编码器有效长度（排除在注意力池中填充词元）\n",
    "在每个解码时间步骤中，解码器上一个时间步的最终隐层状态将用作查询。因此，注意力输出和输入嵌入都连接为循环神经网络解码器的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDeconder(AttentionDecoder):\n",
    "    def __init__(self, vocab_size, embad_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqAttentionDeconder, self).__init__(**kwargs)\n",
    "        self.attention=d2l.AdditiveAttention(num_hiddens, num_hiddens, num_hiddens, dropout)\n",
    "        self.embedding=nn.Embedding(vocab_size, embad_size)\n",
    "        self.rnn=nn.GRU(embad_size+num_hiddens, num_hiddens, num_layers, dropout)\n",
    "        self,dense=nn.Linear(num_hiddens, vocab_size)\n",
    "    \n",
    "    def init_state(self, enc_outputs, enc_valid_lens,  *args):\n",
    "        # outputs的形状为（batch_size, num_steps, num_hiddens)\n",
    "        # hidden_state的形状为\n",
    "        return super().init_state(enc_outputs, *args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Xiaoyao')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcdebb4970db76957bcf11e05672910c0fd8b516a13077c2765e7d5e9fe92ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
